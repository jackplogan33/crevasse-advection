{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70be5f40-52f4-419b-b7e8-8c5d54fe198e",
   "metadata": {},
   "source": [
    "# Constructing dense offset maps from Sentinel-1 SLC's\n",
    "<br>\n",
    "\n",
    "**Author:** Jack Logan <br>\n",
    "**Last Revised:** December 8, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352ac6f-7341-4f5b-b684-0430e0ba2eb4",
   "metadata": {},
   "source": [
    "This notebook uses the ISCE2 script `topsApp.py` to generate dense offset maps (pixel tracking). \n",
    "This notebook is a step-by-step example for generating one offset from a pair of SLC's using the code from `run_isce2.py`. \n",
    "The script is used to run ISCE2 processing on multiple consecutive images from the command line. Follow this notebook for the first offset to understand the process. Then, edit the XML parameters in `run_isce2.py`, and run the rest from the command line.\n",
    "\n",
    "> Refer to the UNAVCO Tutorial from [2021](https://github.com/parosen/Geo-SInC/blob/main/UNAVCO2021/4.4_Offset_stack_for_velocity_dynamics/nb_topsApp_offsets.ipynb) for a conceptual understanding of pixel tracking.\n",
    "\n",
    "**Step 1:** Get Sentinel-1 SLC's from ASF <br>\n",
    "**Step 2:** Get orbits files <br>\n",
    "**Step 3:** Write XML files <br>\n",
    "**Step 4:** Run `topsApp.py`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b88f3b-9dde-471e-8443-8127639dc7cd",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd1f221-fbb0-4bdb-b15f-6aa1ccc9032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import asf_search as asf\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import getpass\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from glob import glob\n",
    "\n",
    "os.chdir('offsets/')  # Move to offsets directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cea77-8bc6-450a-bbd0-c553e0df2474",
   "metadata": {},
   "source": [
    "## Step 1: Getting Sentinel-1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ac460-d77d-4d5a-9eab-56ed0236e35c",
   "metadata": {},
   "source": [
    "Below, we will downloads Sentinl-1 SLC bursts from Alaska Satellite Facility (ASF) using the package `asf_search`. \n",
    "For this example, we are going to generate offsets using the images from August 7, 2018 and August 19, 2018. \n",
    "\n",
    "When running from the command line, you will be prompted to enter the number of images to download and the date you would like to start from. You can easily adapt the script to your own AOI by changing lines 26 and 34 with your own:\n",
    "\n",
    "```python\n",
    "# . . Change this WKT to your desired area\n",
    "aoi = 'POLYGON((38.0336 -69.7358,38.0336 -70.4952,39.6985 -70.4952,39.6985 -69.7358,38.0336 -69.7358))'\n",
    "\n",
    "n_files = int(input('Number of files to download: '))\n",
    "if n_files:\n",
    "    opts = {\n",
    "        'platform':'S1',\n",
    "        'start':str(input('Start Date (YYYY-MM-DD): ')),\n",
    "        'processingLevel':'SLC',\n",
    "        'frame':[830, 834, 936, 938, 939],  # Either remove this line or replace with your ASF frames\n",
    "    }\n",
    "    results = asf.search(intersectsWith=aoi, **opts)[-n_files:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5277b23-2bb5-4cdb-8b66-3a235ddcc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . . Shirase Glacier WKT for asf_search\n",
    "aoi = 'POLYGON((38.0336 -69.7358,38.0336 -70.4952,39.6985 -70.4952,39.6985 -69.7358,38.0336 -69.7358))'\n",
    "n_files = 2  # Only keep two most recent scenes for demo (command line input)\n",
    "\n",
    "# . . Parameters to restrict search\n",
    "opts = {\n",
    "    'platform':'S1',\n",
    "    'start':'2018-08-07',               # Starting date for dem (command line input)\n",
    "    'processingLevel':'SLC',\n",
    "    'frame':[830, 834, 936, 938, 939],  # Scenes known to fully cover Shirase Glacier\n",
    "}\n",
    "results = asf.search(intersectsWith=aoi, **opts)[-n_files:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd7ed5-20bc-4220-935a-6faaab3f62d2",
   "metadata": {},
   "source": [
    "### Start ASF Session and Download the Zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd660769-7917-4853-a78a-45337f250605",
   "metadata": {},
   "source": [
    "Enter your NASA EarthData login credentials here to authenticate and the files will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93bd4c1-d3ef-45e1-aae1-7acffbaa6a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Username: jacklogan@mines.edu\n",
      "Password: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "session = asf.ASFSession()\n",
    "\n",
    "username = input('Username:')\n",
    "password = getpass.getpass('Password:')\n",
    "\n",
    "try:\n",
    "    user_pass_session = asf.ASFSession().auth_with_creds(username, password)\n",
    "except asf.ASFAuthenticationError as e:\n",
    "    print(f'Auth failed: {e}')\n",
    "else:\n",
    "    print('Success!')\n",
    "\n",
    "results.download(path='SAFE/', session=user_pass_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf3cf6-0a01-4896-aa90-a2d8a54da151",
   "metadata": {},
   "source": [
    "## Step 2: Orbit Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9aef60-89d4-44c1-bb2b-bbed1b1e878a",
   "metadata": {},
   "source": [
    "For dense offsets, we need the precise orbits for each pass. This is will allow ISCE2 to accurately coregister the images and compute the dense offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6479f9-4566-451e-bcd4-23a808139260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference time:  2018-08-19 17:56:37\n",
      "Satellite name:  S1A\n",
      "generating a new access token\n",
      "Downloading URL:  https://zipper.dataspace.copernicus.eu/odata/v1/Products(df49cc76-b16a-4ae6-8137-bec03336db8c)/$value\n",
      "Reference time:  2018-08-07 17:56:36\n",
      "Satellite name:  S1A\n",
      "using saved access token\n",
      "Downloading URL:  https://zipper.dataspace.copernicus.eu/odata/v1/Products(1338fde1-ea78-4d6e-85e6-e41c39bcc192)/$value\n"
     ]
    }
   ],
   "source": [
    "# . . Get name of all safe files\n",
    "safe_files = glob(f'./SAFE/*.zip')\n",
    "\n",
    "# . . Download Orbit files to orbit folder\n",
    "os.chdir(f'./orbits')\n",
    "for file in safe_files:\n",
    "    os.system(f'../fetchOrbit.py -i {file[5:-5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877635b3-f3dd-4440-b145-c038bbda8d1b",
   "metadata": {},
   "source": [
    "## Step 3: XML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1888b4-8922-40b8-aef8-0858e4f9d4c8",
   "metadata": {},
   "source": [
    "The XML files tell ISCE2 where to find all of the input files, and what steps to run when processing. The reference and secondary give file paths to the image pairs and orbit data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1581d4-2afe-4397-8059-e86ee74476e1",
   "metadata": {},
   "source": [
    "### Reference and Secondary XMLs\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<component name=\"reference\">\n",
    "    <property name=\"safe\">../SAFE/S1A_IW_SLC__1SSH_20180807T175609_20180807T175636_023143_028391_AE45.zip</property>\n",
    "    <property name=\"output directory\">reference</property>\n",
    "    <property name=\"orbit directory\">../orbits</property>\n",
    "    <property name=\"polarization\">hh</property>\n",
    "</component>\n",
    "```\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<component name=\"secondary\">\n",
    "    <property name=\"safe\">../SAFE/S1A_IW_SLC__1SSH_20180819T175610_20180819T175637_023318_02893F_6998.zip</property>\n",
    "    <property name=\"output directory\">secondary</property>\n",
    "    <property name=\"orbit directory\">../orbits</property>\n",
    "    <property name=\"polarization\">hh</property>\n",
    "</component>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7cb1c-5749-4292-81eb-5d5d8bcbc7b8",
   "metadata": {},
   "source": [
    "### Main Parameter XML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e5ea8-8fff-45ce-acdf-8eced79ae3e9",
   "metadata": {},
   "source": [
    "The main XML, `topsApp.xml`, specifies all the processing steps. We set all InSAR parameters off since we won't be able to unwrap an interferogram in this area. The final block is where we specify the offset parameters.\n",
    "\n",
    "- `Ampcor window height/width`: Size of the patch to correlate between two images\n",
    "- `Ampcor search height/width`: How far to look around the patches\n",
    "- `Ampcor skip height/width`: How far to move before picking a new patch\n",
    "\n",
    "> Note: The specific height/width ratio is dependent on your geometry and the maximum expected displacement between two passes.\n",
    "> Because the range resolution is ~4 time higher than the azimuth resolution, your window widths are typically 3 to 6 times larger than the heights.\n",
    "\n",
    "For our example, we use:\n",
    "- a correlation window of 256x64 provided the best SNR,\n",
    "- maximum displacements over 12 days were no greater than a search of 30x10,\n",
    "- and achieved ~ 100 m ground resolution with a skip of 44x8 \n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<topsApp>\n",
    "    <component name=\"topsinsar\">\n",
    "        <property name=\"Sensor name\">SENTINEL1</property>\n",
    "\n",
    "        <!-- Scene XML files -->\n",
    "        <component name=\"reference\">\n",
    "            <catalog>reference.xml</catalog>\n",
    "        </component>\n",
    "        <component name=\"secondary\">\n",
    "            <catalog>secondary.xml</catalog>\n",
    "        </component>\n",
    "\n",
    "        <!-- The swaths to process -->\n",
    "        <property name=\"swaths\">[2]</property>\n",
    "\n",
    "        <!-- The region of interest -->\n",
    "        <property name=\"region of interest\">[-70.45441464, -69.88490745, 38.29553816,  39.83817435]</property>\n",
    "\n",
    "        <!-- DEM for processing -->\n",
    "        <property name=\"demFilename\">../dem/dem.tiff</property>\n",
    "\n",
    "        <!-- Unset all InSAR processing steps -->\n",
    "        <property name=\"do interferogram\">False</property>\n",
    "        <property name=\"do ESD\">False</property>\n",
    "        <property name=\"do unwrap\">False</property>\n",
    "        <property name=\"do unwrap 2 stage\">False</property>\n",
    "        <property name=\"do ionosphere correction\">False</property>\n",
    "        <property name=\"geocode list\">[]</property>\n",
    "\n",
    "        <!-- Parameters for dense offsets -->\n",
    "        <property name=\"do denseoffsets\">True</property>\n",
    "        <property name=\"Ampcor window width\">256</property>         <!-- Correlation window width  -->\n",
    "        <property name=\"Ampcor window height\">64</property>         <!-- Correlation window height -->\n",
    "        <property name=\"Ampcor search window width\">30</property>   <!-- Horizontal area to search -->\n",
    "        <property name=\"Ampcor search window height\">10</property>  <!-- Vertical area to search   -->\n",
    "        <property name=\"Ampcor skip width\">44</property>            <!-- Horizontal area to jump -->\n",
    "        <property name=\"Ampcor skip height\">8</property>            <!-- Vertical area to jump   -->\n",
    "\n",
    "    </component>\n",
    "</topsApp>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb9e5a-3b60-46b2-8168-f14391d86d80",
   "metadata": {},
   "source": [
    "### Create offset specific directory, write XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a42e5-0ff7-4c13-a177-d6c815268d45",
   "metadata": {},
   "source": [
    "Now we create a directory for the displacement between these two dates. This will contain all the ISCE2 output files to one folder and reduce clutter in the main directories. \n",
    "\n",
    "After making the directory, we move into the and write the XML input files inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd6a087-f867-4b9b-a30f-370c7ace8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . . Make directory, overwrite if existing\n",
    "os.makedirs('../20180807-20180819/', exist_ok=True)\n",
    "\n",
    "# . . Move into directory\n",
    "os.chdir('../20180807-20180819/')\n",
    "\n",
    "# . . topsApp XML input file\n",
    "with open('topsApp.xml', 'w') as f:  # topsApp XML\n",
    "    topsApp = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<topsApp>\n",
    "    <component name=\"topsinsar\">\n",
    "        <property name=\"Sensor name\">SENTINEL1</property>\n",
    "\n",
    "        <!-- Scene XML files -->\n",
    "        <component name=\"reference\">\n",
    "            <catalog>reference.xml</catalog>\n",
    "        </component>\n",
    "        <component name=\"secondary\">\n",
    "            <catalog>secondary.xml</catalog>\n",
    "        </component>\n",
    "\n",
    "        <!-- The swaths to process -->\n",
    "        <property name=\"swaths\">[2]</property>\n",
    "\n",
    "        <!-- The region of interest -->\n",
    "        <property name=\"region of interest\">[-70.45441464, -69.88490745, 38.29553816,  39.83817435]</property>\n",
    "\n",
    "        <!-- DEM for processing -->\n",
    "        <property name=\"demFilename\">../dem/fixed_dem.wgs84</property>\n",
    "\n",
    "        <!-- Unset all InSAR processing steps -->\n",
    "        <property name=\"do interferogram\">False</property>\n",
    "        <property name=\"do ESD\">False</property>\n",
    "        <property name=\"do unwrap\">False</property>\n",
    "        <property name=\"do unwrap 2 stage\">False</property>\n",
    "        <property name=\"do ionosphere correction\">False</property>\n",
    "        <property name=\"geocode list\">[]</property>\n",
    "\n",
    "        <!-- Parameters for dense offsets -->\n",
    "        <property name=\"do denseoffsets\">True</property>\n",
    "        <property name=\"Ampcor window width\">256</property>\n",
    "        <property name=\"Ampcor window height\">64</property>\n",
    "        <property name=\"Ampcor search window width\">30</property>\n",
    "        <property name=\"Ampcor search window height\">10</property>\n",
    "        <property name=\"Ampcor skip width\">44</property>\n",
    "        <property name=\"Ampcor skip height\">8</property>\n",
    "    </component>\n",
    "</topsApp>\"\"\"\n",
    "    f.write(topsApp)\n",
    "\n",
    "#################################################################################\n",
    "                     #### Revert window and skip sizes ####\n",
    "#################################################################################\n",
    "\n",
    "# . . Reference XML input file\n",
    "with open('reference.xml', 'w') as f:\n",
    "    ref = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<component name=\"reference\">\n",
    "    <property name=\"safe\">../SAFE/S1A_IW_SLC__1SSH_20180807T175609_20180807T175636_023143_028391_AE45.zip</property>\n",
    "    <property name=\"output directory\">reference</property>\n",
    "    <property name=\"orbit directory\">../orbits</property>\n",
    "    <property name=\"polarization\">hh</property>\n",
    "</component>\"\"\"\n",
    "\n",
    "    f.write(ref)\n",
    "\n",
    "with open('secondary.xml', 'w') as f:  # Secondary XML\n",
    "    sec = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<component name=\"secondary\">\n",
    "    <property name=\"safe\">../SAFE/S1A_IW_SLC__1SSH_20180819T175610_20180819T175637_023318_02893F_6998.zip</property>\n",
    "    <property name=\"output directory\">secondary</property>\n",
    "    <property name=\"orbit directory\">../orbits</property>\n",
    "    <property name=\"polarization\">hh</property>\n",
    "</component>\"\"\"\n",
    "\n",
    "    f.write(sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d16a4-90fd-4526-bb1d-60392ce11142",
   "metadata": {},
   "source": [
    "## Step 4: Run topsApp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c1ed2-bac1-4d57-9c64-2c3a0dd3b98a",
   "metadata": {},
   "source": [
    "To run `topsApp` from the command line, we have to fix some environment variables. \n",
    "The following code will have to be run in any python script before calling any ISCE scripts from the command line.\n",
    "This code can be found in the `setup_environment` function in `run_isce2.py`. To properly setup the environment on your machine, change lines 82 and 83:\n",
    "```python\n",
    "def setup_environment():\n",
    "    \"\"\"Setup ISCE2 environment\"\"\"\n",
    "    conda_path = '/home/jovyan'  # Your anaconda directory\n",
    "    isce_env = 'envs/isce2'      # Where your ISCE2 environment is saved\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9955e01e-b122-4303-9352-b3ac58a8ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . . Root paths\n",
    "conda_path = '/home/jovyan'  # Where all anaconda environments are saved\n",
    "isce_env = 'envs/isce2'      # ISCE2 environment created in Notebook 1\n",
    "\n",
    "isce_home = f'{conda_path}/{isce_env}/lib/python3.8/site-packages/isce'\n",
    "isce_stack = f'{conda_path}/{isce_env}/share/isce2'\n",
    "isce_lib_path = f\"{conda_path}/{isce_env}/lib\"\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{isce_lib_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "os.environ['ISCE_HOME'] = isce_home\n",
    "os.environ['ISCE_STACK'] = isce_stack\n",
    "os.environ['ISCE_ROOT'] = f'{conda_path}/{isce_env}/lib/python3.8/site-packages'\n",
    "\n",
    "# Update PATH and PYTHONPATH\n",
    "path_components = [\n",
    "    f\"{isce_home}/bin\",\n",
    "    f\"{isce_home}/applications\",\n",
    "    f\"{isce_stack}/topsStack\",\n",
    "    os.environ.get('PATH', '')\n",
    "]\n",
    "os.environ['PATH'] = ':'.join(filter(None, path_components))\n",
    "\n",
    "pythonpath_components = [\n",
    "    f'{conda_path}/{isce_env}/lib/python3.8/site-packages',\n",
    "    isce_home,\n",
    "    isce_stack,\n",
    "    f\"{isce_home}/applications\",\n",
    "    f\"{isce_home}/components\",\n",
    "    os.environ.get('PYTHONPATH', '')\n",
    "]\n",
    "os.environ['PYTHONPATH'] = ':'.join(filter(None, pythonpath_components))\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '8'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8e7231-9c7f-4f77-adb6-6cff0cb97d9b",
   "metadata": {},
   "source": [
    "Now we run this cell and wait. \n",
    "With the default parameters for this example, the runtime is ~2 hours per offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646915d8-6d78-47fc-984d-cf1baebbf0e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m      4\u001b[0m     cmd, \n\u001b[1;32m      5\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     cwd\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetcwd(),\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m process\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cmd = f\"conda run -n isce2 topsApp.py topsApp.xml\"\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    cmd, \n",
    "    shell=True, \n",
    "    text=True,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env=os.environ,\n",
    "    cwd=os.getcwd(),\n",
    ")\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a88089-ec1a-420e-ab30-e3b04049a78c",
   "metadata": {},
   "source": [
    "## Step 5: Trim outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3aee82-9af1-4561-9d84-57ee9651df1f",
   "metadata": {},
   "source": [
    "ISCE2 generates a lot of extra files that are not used in the final product.\n",
    "To help keep our files clean and small in a cloud computing environment, we are going to trim the outputs and delete extra files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a8d1019-0885-4a8a-b6c3-188fc897edb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24G\t.\n",
      "4.3G\t../SAFE\n"
     ]
    }
   ],
   "source": [
    "!du -hs .\n",
    "!du -hs ../SAFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb89ae-bf21-490e-86c6-6b880be311f9",
   "metadata": {},
   "source": [
    "As you can see, the current directory for processing is now 24 GB, significantly larger than the original 4.3 GB from the SAFE files.\n",
    "\n",
    "This happens because ISCE2 copies all of the bursts several times. The cell below will remove all files except the XML files, the log file, and the final `merged` folder, which stores the final outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f442653f-1293-4aed-8f45-f324c6729de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9G\t.\n"
     ]
    }
   ],
   "source": [
    "# . . Keep these files\n",
    "keep_files = {\n",
    "    \"isce.log\",\n",
    "    \"reference.xml\",\n",
    "    \"secondary.xml\",\n",
    "    \"topsApp.xml\"\n",
    "}\n",
    "# . . Keep this directory\n",
    "keep_dirs = {\"merged\"}\n",
    "\n",
    "# . . Clean current directory\n",
    "processing_dir = os.getcwd()\n",
    "\n",
    "# . . Remove rest of processing directory\n",
    "for entry in os.listdir():\n",
    "    full_path = os.path.join(processing_dir, entry)\n",
    "\n",
    "    if os.path.isdir(full_path):\n",
    "        if entry not in keep_dirs:\n",
    "            print(f\"Deleting directory: {full_path}\")\n",
    "            shutil.rmtree(full_path)\n",
    "\n",
    "    else:\n",
    "        if entry not in keep_files:\n",
    "            print(f\"Deleting file: {full_path}\")\n",
    "            os.remove(full_path)\n",
    "\n",
    "# . . Remove DEM, reference and secondary SLCs from merged\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/dem*')]\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/reference*')]\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/secondary*')]\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/*rdr*')]\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/filt_dense_offsets.bil*')]\n",
    "\n",
    "!du -hs ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe74f6-4511-4d7c-8910-64f19421bebe",
   "metadata": {},
   "source": [
    "Removing all the extra files alone reduced the amount of data by 10 times. \n",
    "\n",
    "Additionally, the \"Region of Interest\" property from the XML does not actually clip the data. Below, we will crop the output files, save in netCDF, and delete the original files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64b3e869-a124-48de-a35c-8030c80c3726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping Offset output files...\n",
      "Offsets Cropped\n",
      "Cropping SNR files...\n",
      "SNR Cropped\n",
      "Cropping COV files\n",
      "COV Cropped\n"
     ]
    }
   ],
   "source": [
    "def crop_denseoff(file):\n",
    "    xmin, xmax = 1.345e6, 1.43e6\n",
    "    ymin, ymax = 1.646e6, 1.76e6\n",
    "    \n",
    "    da = xr.open_dataarray(file, engine='rasterio').rio.reproject(3031, nodata=np.nan)\n",
    "    da = da.sel(x=slice(xmin, xmax), y=slice(ymax, ymin))\n",
    "    return da\n",
    "\n",
    "print(\"Cropping Offset output files...\")\n",
    "# crop dense offsets, save as netCDF, remove .bil*\n",
    "crop_denseoff(\n",
    "    os.path.join(processing_dir, 'merged', 'dense_offsets.bil.geo')\n",
    ").to_netcdf(os.path.join(processing_dir, 'merged', 'dense_offsets.nc'))\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/dense_offsets.bil*')]\n",
    "print(\"Offsets Cropped\")\n",
    "\n",
    "print(\"Cropping SNR files...\")\n",
    "# crop SNR, save as nc, remove .bil*\n",
    "crop_denseoff(\n",
    "    os.path.join(processing_dir, 'merged', 'dense_offsets_snr.bil.geo')\n",
    ").to_netcdf(os.path.join(processing_dir, 'merged', 'dense_offsets_snr.nc'))\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/dense_offsets_snr.bil*')]\n",
    "print(\"SNR Cropped\")\n",
    "\n",
    "print(\"Cropping COV files\")\n",
    "# crop COV, save as nc, remove .bil*\n",
    "crop_denseoff(\n",
    "    os.path.join(processing_dir, 'merged', 'dense_offsets_cov.bil.geo')\n",
    ").to_netcdf(os.path.join(processing_dir, 'merged', 'dense_offsets_cov.nc'))\n",
    "[os.remove(f) for f in glob(f'{processing_dir}/merged/dense_offsets_cov.bil*')]\n",
    "print('COV Cropped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7d5e7ef-b858-4324-a96e-3fc4ad371fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2G\t.\n"
     ]
    }
   ],
   "source": [
    "!du -hs ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f2b837-1fe9-4c42-9779-34519b494600",
   "metadata": {},
   "source": [
    "## Step 6: Automate Offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aff9b1-8190-4fe4-a4f7-75f21c2b7e26",
   "metadata": {},
   "source": [
    "Now that we've gone through this process, we can run the python script `run_isce2.py` from the command line. \n",
    "It will prompt you to input your NASA EarthData credentials, the number of SAFE files to download, and a start date. \n",
    "It will then automatically download the Orbit files required and begin the processing. \n",
    "\n",
    "In the event there is a missing SLC, creating a 24 day repeat pass, the script automatically accounts for thie change and doubles the search window sizes. \n",
    "We recommend limiting your search windows to just larger than the maximum observed or expected displacement, found empirically.\n",
    "\n",
    "After each offset is computed, we delete all the extra files left over by the processing, then clip the data to your ROI. Unfortunately this last step is hardcoded, and requires another change in values.\n",
    "\n",
    "#### **To customize for your own region:**\n",
    "1. Change the AOI wicket on line 26 and the frames on line 34\n",
    "3. Adjust the `region of interest` property on line 281.\n",
    "4. Change the Ampcor window properties on lines 295-301.\n",
    "5. Change the clipping limits in line 396. <br>\n",
    "   *Note the decreasing y-axis when slicing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcdc148-5aae-4f07-afcb-31e3038d5485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
